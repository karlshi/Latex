
%% overclocking_tcad.tex
%% by Kan Shi


%\usepackage{times,epsfig,amsfonts,bm}
%\usepackage{pdflscape}
%\usepackage{afterpage}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{fmtcount,threeparttable, setspace, nicefrac}
%\usepackage[dvips]{color}
%\usepackage{amsthm}

\documentclass[journal]{IEEEtran}
\usepackage{cite}

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{../Figures/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.eps}
\fi
\usepackage{epstopdf}


\usepackage{multirow}
\usepackage{amssymb}
\usepackage[cmex10]{amsmath}
%\usepackage{algorithmic}
%\usepackage{array}
\usepackage[tight,footnotesize]{subfigure}
\usepackage{stfloats}
\usepackage{threeparttable}



\begin{document}

\title{Imprecise Computing in Datapath Design: An Overclocking Approach}
%Imprecise Computing in Datapath Design: Precision Loss or Timing Violations}
%Imprecise Datapath Design: An Overclocking Approach
%Imprecise Computation Through Overclocking

\author{Kan~Shi,
        David~Boland,~\IEEEmembership{Member,~IEEE,}
        and~George~A.~Constantinides,~\IEEEmembership{Senior~Member,~IEEE}% <-this % stops a space
	\thanks{K. Shi, D. Boland and G. A. Constantinides are with the Department of Electrical and Electronic Engineering, Imperial College London, London, SW7 2BT, U.K. (email: \{k.shi11, david.boland03, g.constantinides\}@imperial.ac.uk)}%
	%\thanks{M. Shell is with the Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
\thanks{Manuscript received April 19, 2005; revised December 27, 2012.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~11, No.~4, December~2012}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
As process scaling introduces significant performance variations, releasing the tight accuracy requirement rather than covering all possible worst cases would potentially offer the greater freedom to create a design with better performance or energy efficiency. In this paper, we compare two different approaches that could trade accuracy for performance. One is the traditional approach where the precision used in the datapath is limited to meet a target latency. The other is a proposed new approach to boost the clock frequency by simply allowing the datapath to operate without timing closure. We demonstrate analytically and experimentally that on average, our approach obtains either smaller errors or faster operating frequencies in comparison to the traditional approach, since the worst case caused by timing violations only happens rarely, while the precision loss results in errors to all data. We also show that for embedded applications where silicon area is also a limited factor, using the proposed approach on simple arithmetic primitives could achieve better accuracy or performance than using the traditional method on advanced operators.
\end{abstract}

\begin{IEEEkeywords}
Overclocking, Imprecise Computing, Numerical Analysis.
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{C}{ircuit} performance has increased tremendously over the past decades, with the continuous scaling of CMOS technology to the nanometer regime. However, the drastic variations introduced by higher integration densities is anticipated to be the major obstacle when designing reliable, high performance circuits. Although hardware designers traditionally employ safety margins to ensure a uniform functionality across a variety of possible working environments, there are two major problems if we keep using this approach nowadays. One is that the uniform performance should no longer be expected in the future, because the highly scaled CMOS devices would inevitably exhibit probabilistic or statistical behavior. The other problem is that covering all possible worst cases would become increasingly difficult, expensive and result in large yield loss because of the variation.

In addition, we also notice that the clock frequency is no longer scaling significantly with time, and the dark silicon becomes an important issue in circuit design since energy efficiency is considered to be the main limitation for performance improvement in embedded or mobile applications~\cite{FutureMicroprocessor2005,DarkSilicon2011}. In this case, designers tend to employ multi-core processors, heterogeneous systems and massive parallel accelerators to sustain the performance scaling.

Specifically, large volume of current studies has demonstrated that, significant performance gains can be achieved by FPGA-based accelerators over software designs across a wide range of applications. However, one of the major factors that limits the performance of these accelerators is that they typically run at much lower clock frequencies than general purpose processors (GPPs) or GPUs. Furthermore, timing analysis tools typically report a very conservative clock frequency to guarantee ``safe'' operations. This substantially limits the potential performance of the device to a further extent.

In order to boost the operating frequency of a datapath, the standard approaches are either to heavily pipeline the design, or reduce the precision throughout the datapath. For the former method, it should be noted that pipelining will not tend to reduce the circuit latency. Actually the latency in terms of clock cycles will increase. As a result, this method will not be applicable to many embedded applications, which typically have strict latency requirements, or in datapath containing feedback where C-slow retiming is inappropriate. For the second approach, reducing the datapath precision would subtract out the latency, but at the cost of introducing quantization errors into the design. For hardware platforms such as FPGAs or ASICs, research into exploiting the potential benefits of using the minimum precision necessary to satisfy a design specification has been an extensive topic~\cite{GeorgeDT11}, as it is possible to use a number system with customized precisions on these platforms. A brief review of existing approaches in this area will be discussed in Section~\ref{Section_Background_WL}.

Unfortunately, neither of the conventional approaches tends to remove the conservative safety margin. In recent years, we have seen a growth of research that explores the potential power or performance benefits that can be obtained when relaxing the variation-induced guard-bands~\cite{NewBox2004}. The detailed review of this research field will be presented in Section~\ref{Section_Background_Imprecise}.

In this paper, we describe an alternative circuit design methodology when considering tradeoffs between accuracy, performance and area. We suggest that for certain applications it is beneficial to move away from the traditional model of creating a conservative design that is guaranteed to avoid timing violations. Instead, it may be preferable to create a design in which timing violations may occur, under the knowledge that they only occur rarely because specific input patterns are required to generate the worst case errors. This paper elaborates on the prior work [xxx], with a further enrichment by incorporating silicon area as another evaluation metric for the trade-offs. In this work, we explicitly demonstrate the optimum design choice of the basic arithmetic operators (i.e. binary adders) under various design constraints. A summary of the main contributions of this work are as follows:

The rest of this paper is organized as follows. It first reviews in detail the current literature regarding the existing approaches that are used in word-length optimization and approximate datapath design in Section~\ref{Section_Background}. The analytical error models for two different adder structures: RCA and CSA, are discussed in Section~\ref{section_RCA} and Section~\ref{Section_CSA}, respectively. The decision of the optimum adder structure under the accuracy-performance-area trade-offs, is then put forward in Section~\ref{Section_OptimumAdder}. It is followed by a description of the probabilistic error models for CCM in Section~\ref{section_CCM}. Section~\ref{Section_ExpSetup} discusses a practical experimental setup to verify our models and test the proposed design methodology. The demonstration of the benefits of our proposed approach is then demonstrated in Section~\ref{Section_Experiments}. Finally, Section~\ref{Section_Conclusion} discusses the conclusions of this work and the possible research directions in the future.

\section{Background}\label{Section_Background}
\subsection{Word-length Optimization}\label{Section_Background_WL}
There exists a significant amount of work demonstrating that optimizing the precision used throughout the datapath would bring a substantial benefits on clock speed, silicon area and power consumption. As loosing precisions causes truncation or overflow errors, a main stream of research focus on analyzing errors generated from using the minimum precision and providing bounds of inputs to ensure the overall error are within the given accuracy specification. Current literatures report two major approaches in this area: the simulation-based approach [xxx] and the analytical-based approach such as interval arithmetic~\cite{moore1966IA}, affine arithmetic~\cite{de2004affine}, Satisfiability-Modulo Theories~\cite{SAT2010} and polynomial representations~\cite{Boland2011TCAD}. A detailed summary of these methods can be found in~\cite{GeorgeDT11, lee2006accuracy}.

\subsection{Imprecise Circuit Design Methodology}\label{Section_Background_Imprecise}
Note that, however, the choice of precision is not the only source of errors when designing a datapath. Recently we have seen a growth of parallel streams of research which aim at exploring alternative methods to trade accuracy for design efficiency. This strand of research is motivated by the drastic performance variations introduced by process scaling.

%it becomes increasingly difficult and expensive to cover all possible worst cases when designing circuits, because the rapid scaling of technology introduces drastic variations on circuit performance.

To address this problem, the international technology roadmap for semiconductors (ITRS07) pointed out that extra benefits of manufacturing, test, power and timing can be obtained if the tight requirement of absolute correctness is released for devices and interconnects~\cite{semiconductor2007international}. This topic is expected to be of growing importance in the future, because we will face new challenges as the technology scales further.

According to the existing literatures, one way to operating circuits beyond the deterministic region is to relax the design constraints and the guard-bands that are conventionally used to avoid the worst cases. Research in this area is based on the observations that in practice, the worst case happens rarely. A series of work known as ``Better-Than-Worst-Case (BTWC) Design'' introduced a universal framework to push the circuit performance to its limits~\cite{BetterThanWS2005}. In general, a BTWC design is composed of cores and checkers. The cores are operated with high performance by eliminating the guard-bands, meanwhile the possible timing errors are diagnosed by the checkers. Optionally the system can be recovered at the observation of errors. As an exemplary design, the Razor project~\cite{Razor2003,Razor2004} was proposed to shave the conservative timing margins by overscaling the supply voltage and clock frequency, while monitoring the output error rates by utilizing a self-checking circuit. This work demonstrated that the benefits brought by removing the safe margin outweigh the cost of monitoring and recovering from errors. For example, $22\%$ or over $30\%$ power consumption can be saved with $\sim0.01\%$ or $\sim1\%$ error rates at the output, respectively. Related work includes a similar frequency overscaling technique by operating circuits slightly slower than the critical path delay with dedicated checker circuits to ensure that timing errors will not occur~\cite{TEAtime2004}, or developing timing analysis tools that decide the optimum operating frequencies in the non-deterministic region due to process variation~\cite{ProbabilisticSTA}.

Another key observation that has been made from existing work is that the worst-case normally happens with specific input patterns. Meanwhile, many studies have shown that errors can be potentially tolerated in various applications, e.g. DSP applications concerning human perceptions. In this case, an even greater performance/power benefits can be achieved, as the design overhead of the checkers can be eliminated. Current research in this area is focusing on designing probabilistic or imprecise circuits that employ techniques from either software level or hardware level. For instance at the programming language level, a tool was proposed~\cite{EnerJ2011Uwash,Truffle2012Uwash} to divide the program into the precise parts and the approximate parts, both of which are mapped to different hardware with different speed-grades, supply voltages etc. This technique enables a relatively high service quality, in the meantime energy reduction can be obtained due to approximation. Similar ideas have also been applied on hardware directly. As an example, Palem et~al.~\cite{NonUniformScaling} described a non-uniform voltage scaling technique for the ripple carry adder. In this study, multiple voltage regions are employed for different bits along a carry chain. That is, higher voltage would be applied for computations generating most significant bits, and vice versa.

While the aforementioned literatures take advantage of the fact that only specific input patterns could cause timing errors, research on imprecise architectures take this one step further by designing simplified circuits for peformance/energy efficiency. For example, Lu et~al. proposed a ``shrinking'' datapath that can be utilized to mimic and speculate the original logic functions~\cite{IntelSpeeding}. Kulkarni et.al. described an underdesigned $2\times2$ multiplier unit, of which the worst case was replaced by a normal case based on the straight-forward Karnaugh-Map analysis~\cite{Undersigned2x2multiplier}. In both cases, reduction of area and power consumption can be achieved with the cost of accuracy. In addition, Gupta et~al. developed approximate adders at the transistor level and compared the energy efficiency of their proposed architectures over truncation of input word-length of conventional structures~\cite{Gupta2013TransCADICS}.

However, the limitations of the existing work should not be neglected. Firstly, some techniques, such as employing several voltages regions within a ripple carry adder, are not practical to achieve in real situations, and the overhead of applying this technique is not discussed. Secondly, many studies propose alternative architectures, which means the adaptability of this kind of technique is limited on current hardware platforms. More importantly, correct results would never be obtained as long as the approximate architectures were employed. Thirdly, the link between the probability of output correctness and energy savings or performance improvements are rarely analyzed from current research.

In this work, we propose an alternative methodology to remove these limitations. It is argued in this paper that our method can be easily applied onto existing hardware arithmetic operators such as RCAs and CCMs with almost no hardware overhead. We support this hypothesis experimentally and analytically by presenting theoretical probabilistic error models.


%, by simply allowing timing violations to happen. We compare to the situation where a specific timing requirement is met by reducing datapath precisions. Instead of simply performing experiments, we also present analytical models of errors generated due to both timing violation and precision loss.

%In addition, a variety of research aimed at designing imprecise architectures to get rid of the worst cases.

%such as non-uniform voltage scaling or imprecise architectures for computations.

%achieving a uniform circuit performance and 100\% accuracy becomes increasingly difficult and expensive, especially when technology shrinks to the nanometer regime.


\section{Ripple Carry Adder}\label{section_RCA}
\subsection{Adder Structures in FPGAs}
%something about fast carry logic in FPGA and pictures
Adders serve as a key building block for arithmetic operations. Generally speaking, the ripple carry adder (RCA) is the most straightforward and widely used adder structure. As such, the philosophy of our approach is first exemplified with the analysis of a RCA. We later describe how this methodology can be extended to other arithmetic operators in Section~\ref{section_CCM} by discussing  the CCM that is commonly used in DSP applications and numerical algorithms.

Typically the maximum frequency of a RCA is determined by the longest carry propagation. Consequently, modern FPGAs offer built-in architectures for very fast ripple carry addition. For instance, the Altera Cyclone series uses fast tables~\cite{AlteraCyclone} while the Xilinx Virtex series employs dedicated multiplexers and encoders for the fast carry logic~\cite{Virtex6}. Figure~\ref{FPGA adder} illustrates the structure of an $n$-bit RCA, which is composed of $n$ serial-connected full adders (FAs) and utilizes the internal fast carry logic of the Virtex-6 FPGA.

While the fast carry logic reduces the time of each individual carry-propagation delay, the overall delay of carry-propagation will eventually overwhelm the delay of sum generation of each LUT with increasing operand word-lengths. For our initial analysis, we assume that the carry propagation delay of each FA is a constant value $\mu$, which is a combination of logic delay and routing delay, and hence the critical path delay of the RCA is $\mu_{RCA}=n\mu$, as shown in~Figure~\ref{FPGA adder}. For an $n$-bit RCA, it follows that if the sampling period $T_S$ is greater than $\mu_{RCA}$, correct results will be sampled. If, however, $T_S<\mu_{RCA}$, intermediate results will be sampled, potentially generating errors.
%
\begin{figure}[t]
  \centering
  \includegraphics[width=3in]{./Figures/FastCarryLogic3.eps}
  %  \vspace{-2ex}
  \caption{An $n$-bit ripple carry adder in the Xilinx Virtex-6 FPGA.}
  \label{FPGA adder}
  %\vspace{-3.5ex}
\end{figure}

In the following sections, we consider two methods that would allow the circuit to run at a frequency higher than $1/{T_S}$. The first is a traditional circuit design approach where operations occur without timing violations. To this end, the operand word-length is truncated in order to meet the timing requirement. This process results in truncation or roundoff error. In our proposed new scenario, circuits are implemented with greater word-length, but are clocked beyond the safe region so that timing violations sometimes occur. This process generates ``overclocking error''.

 %We note that it may be possible to operate at a higher clock frequency than $1/T_{S}$ by using alternative adder structures, such as carry-select adder and carry lookahead adder. However, the effectiveness of these techniques is limited in FPGAs due to the build-in structures for the acceleration of carry propagation. In addition, such techniques will require substantial extra silicon area.

\subsection{Probabilistic Model of Truncation Error}\label{section_RCA_TruncationError}
For ease of discussion, we assume that the input to our circuit is a fixed point number scaled to lie in the range $[-1,1)$. For our initial analysis, we assume every bit of each input is uniformly and independently generated. However, this assumption will be relaxed in Section~\ref{Section_Experiments} where the predictions are verified using real image data. The errors at the output are evaluated in terms of the absolute value and the probability of their occurring. These two metrics are combined as the error expectation.

If the input signal of a circuit is $k$ bits, truncation error occurs when the input signal is truncated from $k$ bits to $n$ bits. Under this premise, the mean value of the truncated bits at signal input ($E_{Tin}$) is given by (\ref{ET at single input}).
%
%\vspace{-1ex}
\begin{eqnarray}\label{ET at single input}
%\small
%\footnotesize
%\scriptsize
  \begin{split}
    E_{Tin} &= \frac{1}{2}\sum_{i=n+1}^{k}2^{-i}\\
    &= 2^{-n-1}-2^{-k-1}
%  \vspace{-1ex}
  \end{split}
\end{eqnarray}
%\normalsize

Since we assume there are two mutually independent inputs to the RCA, the overall expectation of truncation error for the RCA is given by (\ref{TruncationError}).
%
%\vspace{-1ex}
\begin{eqnarray}\label{TruncationError}
%\small
%\footnotesize
  E_T=\left\{
    \begin{matrix}
        2^{-n}-2^{-k}, & \textrm{if $n<k$}\\
        0, & \textrm{otherwise}
    \end{matrix}
  \right.
\end{eqnarray}
%\normalsize

\subsection{Probabilistic Model of Overclocking Error}
\subsubsection{Generation of Overclocking Error}\label{subsub:Generation of Overclocking Error}
For a given $T_S$, the maximum length of error-free carry propagation is described by~(\ref{Max.length of carry chain}), where $f_S$ denotes the sampling frequency.
%
%\vspace{-1ex}
\begin{eqnarray}\label{Max.length of carry chain}
%\small
%\footnotesize
%\scriptsize
    b := \left\lceil \frac{T_S}{\mu} \right\rceil= \left\lceil \frac{1}{\mu\cdot f_S}\right\rceil
\end{eqnarray}
%\normalsize

However, since the length of an actual carry chain during execution is dependent upon input patterns, in general, the worst case may occur rarely. To determine when this timing constraint is not met and the size of the error in this case, we expand standard results~\cite{DigitalICDesign} to the following statements, which examine carry generation, propagation and annihilation, as well as the corresponding summation results of a single bit $i$, according to the relationship between its input patterns $A_i$ and $B_i$:
%
 \begin{itemize}
   \item If $\!A_i\!=\!B_i\!=1$, a new carry chain is generated at bit $i$, and $S_i\!=\!C_{i-1}$;
   \item If $A_i\neq B_i$, the carry propagates for this carry chain at bit $i$, and $S_i\!=\!0$;
   \item If $\!A_i\!=\!B_i$, the current carry chain annihilates at bit $i$, and $S_i\!\!=\!\!1$.
 \end{itemize}

\subsubsection{Absolute Value of Overclocking Error}
For an $n$-bit RCA, let $C_{tm}$ denote the carry chain generated at bit $S_t$ with the length of $m$ bits. For a certain $f_S$, the maximum length of error-free carry propagation, $b$, is determined through (\ref{Max.length of carry chain}). The presence of overclocking error requires $m>b$. Since the length of carry chain cannot be greater than $n$, parameters $t$ and $m$ are bounded by (\ref{t_RCA}) and (\ref{m_RCA}):
%
%\vspace{-1ex}
\begin{align}
%\small
%\footnotesize
%\scriptsize
  \label{t_RCA}  0&\leq t \leq n-b\\
    b&<m\leq n+1-t      \label{m_RCA}
%    \vspace{-1ex}
\end{align}
%\normalsize

For $C_{tm}$, correct results will be generated from bit $S_t$ to bit $S_{t+b-1}$. Hence the absolute value of error seen at the output, normalized to the MSB ($2^n$), is given by (\ref{Output Error}), where $\hat{S_i}$ and $S_i$ denote the actual and error-free output of bit $i$ respectively.
%
%\vspace{-1ex}
\begin{eqnarray}\label{Output Error}
%\small
%\footnotesize
    e_{tm}=\frac{\left|\sum_{i=t+b}^{n}(S_i-\hat{S_i})\cdot 2^i\right|}{2^n}
%    \vspace{-1ex}
\end{eqnarray}
%\normalsize

$S_i$ and $\hat{S_i}$ can be determined using the equations from the previous statements in Section~\ref{subsub:Generation of Overclocking Error}. In the error-free case, the carry will propagate from bit $S_t$ to bit $S_{t+m-1}$, and we will obtain $S_{t+b}=S_{t+b+1}=\cdots=S_{t+m-2}=0$ for carry propagation, and $S_{t+m-1}=1$ for carry annihilation. However, when a timing violation occurs, the carry will not propagate through all these bits. Substituting these values into (\ref{Output Error}) yields (\ref{etm}). Interestingly, the value of overclocking error has no dependence on the length of carry chain $m$.
%
%Instead, $\hat{S}_{t+b}=\hat{S}_{t+b+1}=\cdots=\hat{S}_{t+m-2}=1$ and $\hat{S}_{t+m-1}=0$. It is worth noting that these expressions are obtained based on the assumption that all the internal bits are initialized to zero.
%
\begin{eqnarray}\label{etm}
%\small
%\footnotesize
  \begin{split}
    e_{tm} &= \frac{\left|2^{t+m-1}-2^{t+m-2}-\dots-2^{t+b}\right|}{2^n}\\
    &= 2^{t+b-n}
  \end{split}
\end{eqnarray}
%\normalsize

\subsubsection{Probability of Overclocking Error}
The carry chain $C_{tm}$ occurs when there is a carry generated at bit $t$, a carry annihilated at bit $t+m-1$ and the carry propagates in between. Consequently, its probability $P_{tm}$ is given by (\ref{Ptm_RCA}).
%
%\vspace{-1ex}
\begin{eqnarray}\label{Ptm_RCA}
%\small
%\footnotesize
  P_{tm}=P_{(A_t=B_t=1)}P_{(A_{t+m-1}=B_{t+m-1})}\cdot \prod_{i=t+1}^{t+m-2}P_{(A_i\neq B_i)}
%  \vspace{-1ex}
\end{eqnarray}
%\normalsize
Under the assumption that $A$ and $B$ are mutually independent and uniformly distributed, we have $P_{(A_i=B_i=1)}=1/4$, $P_{(A_i\neq B_i)}=1/2$ and $P_{(A_i=B_i)}=1/2$, so $P_{tm}$ can be obtained by~(\ref{Ptm2}). Note that~(\ref{Ptm2}) takes into account the carry annihilation always occurs when $t+m-1=n$.
%\vspace{-1ex}
%
\begin{eqnarray}\label{Ptm2}
%\small
%\footnotesize
    P_{tm}=\left\{\begin{array}{ll}
      (1/2)^{m+1} & \textrm{if $t+m-1<n$}\\
      (1/2)^{m} & \textrm{if $t+m-1=n$}
    \end{array} \right.
%    \vspace{-1ex}
\end{eqnarray}
%\normalsize

\subsubsection{Expectation of Overclocking Error}
Expectation of overclocking error can be expressed by (\ref{Eo_exp}).
%
%\vspace{-1ex}
\begin{eqnarray}\label{Eo_exp}
%\small
%\footnotesize
%\scriptsize
    %E_O=\sum_{t=0}^{n-b}\sum_{m=b+1}^{n-t+1}P_{tm}\cdot e_{tm}
    E_O=\sum_{t}\sum_{m}P_{tm}\cdot e_{tm}
%    \vspace{-1ex}
\end{eqnarray}
%\normalsize
%
Using $P_{tm}$ and $e_{tm}$ from (\ref{etm}) and (\ref{Ptm2}) respectively, $E_O$ can be obtained by (\ref{Eo}).
%\vspace{-1ex}
\begin{eqnarray}\label{Eo}
%\small
%\footnotesize
      E_O=\left\{
        \begin{matrix}
            2^{-b}-2^{-n-1}, & \textrm{if $b\leq n$}\\
            0, & \textrm{otherwise}
        \end{matrix}
      \right.
%      \vspace{-1ex}
\end{eqnarray}
%\normalsize

\subsection{Comparison between Two Scenarios}\label{section_RCA_Comparison}
In the traditional scenario, the word-length of RCA must be truncated, using $n=b-1$ bits, in order to meet a given $f_S$. The error expectation is then given by (\ref{Eo_trad_RCA}).
%
\begin{eqnarray}\label{Eo_trad_RCA}
%\small
%\vspace{-1ex}
%\footnotesize
%\scriptsize
  E_{trad}=2^{-b+1}-2^{-k}
  \vspace{-1ex}
\end{eqnarray}
%\normalsize
Overclocking errors are allowed to happen in the second scenario, therefore the word-length of RCA is set to be equal to the input word-length, that is, $n=k$. Hence we obtain (\ref{Eo_new_RCA}) according to (\ref{Eo}).
%
\begin{eqnarray}\label{Eo_new_RCA}
%\vspace{-1ex}
%\small
%\footnotesize
%\scriptsize
  E_{new}=2^{-b}-2^{-k-1}
%  \vspace{-1ex}
\end{eqnarray}
%\normalsize

%
Comparing (\ref{Eo_new_RCA}) and (\ref{Eo_trad_RCA}), we have (\ref{Comparison RCA}). This equation indicates that by allowing timing violations, the overall error expectation of RCA outputs drops by a factor of 2 in comparison to traditional scenario. This provides the first hint that our approach is useful in practice.
%
\begin{eqnarray}\label{Comparison RCA}
%\vspace{-1ex}
%\small
%\footnotesize
%\scriptsize
  \frac{E_{new}}{E_{trad}}=\frac{2^{-b}-2^{-k-1}}{2^{-b+1}-2^{-k}}=\frac{1}{2}
\end{eqnarray}



\section{Carry Select Adder}\label{Section_CSA}
\subsection{Introduction}
Since the delay of RCA is determined by the length of carry chain, alternative adder architectures, such as carry select adder (CSA) have been proposed to boost performance. For CSA, the carry chain is divided into multiple overlapped sections to increase the operating speed, as shown in Fig.~\ref{Fig_CSA_Structure}. There are multiple stages within a CSA. Each stage contains two RCAs and two multiplexers, as seen in Fig.~\ref{Fig_CSA_SingleStage}. For a given input, two additions are performed simultaneously where the carry input is zero and one respectively. One of these two results is then selected according to the actual carry input. Although this structure brings performance benefits, it costs extra hardware resources compared to a standard RCA because the carry chain is duplicated. Furthermore, in FPGA technology, multiplexers are expensive. Due to this reason, we explore the trade-offs between silicon area, accuracy and performance of RCA and CSA in this section.

\begin{figure}[htbp]
    \begin{minipage}[b]{.49\textwidth}
        \centering
        \subfigure[The structure diagram of a CSA with $s$ stages.]{
            \includegraphics[width=0.95\textwidth]{./Figures/CSA_structure.pdf}
            \label{Fig_CSA_Structure}
        }\vspace{-2ex}
        \subfigure[The structure of the $i^{th}$ stage in the CSA.]{
        %\centering
            \includegraphics[width=2.4in]{./Figures/CSA_SingleStage.pdf}
            \label{Fig_CSA_SingleStage}
        }
    \end{minipage}
    \centering
    \caption{The structure diagram of CSA.}
\end{figure}

%, silicon area is incorporated as the third metric besides accuracy and performance. We explore the trade-offs between these three factors for both adder structures. The objective is to demonstrate the best design choice for a given set of design constraints.

%\begin{figure*}[t]
%  \centering
%  \includegraphics[width=\textwidth]{./Figures/CSAstructure.pdf}
%  \caption{Structure of an $s$-stage CSA.}
%\end{figure*}

%In addition to RCA, there are several

\subsection{Timing Models for Carry Select Adder}
We initially model the CSA in order to understand the relationship between the operating frequency and maximum word-length of the CSA. This information can then be employed to determine the truncation error based on the models presented in Section~\ref{section_RCA_TruncationError}.

%In a CSA, the stage delay refers to the combination of the carry propagation delay and the delay of multiplexing the carry output of the giv stage.

In our analysis, the stage delay of stage $i$ in the CSA refers to the combination of the $i$-bit carry propagation delay and the delay of multiplexing the carry output. For a CSA with $s$ stages~($s\geqslant 2$), let the stage delay be denoted by $d_{s-1},~\dots,~d_{0}$, where $d_{s-1}$ and $d_{0}$ represent the delay of the most significant and the least significant stages, respectively. We still follow the aforementioned assumption that the critical path delay of the CSA is due to carry propagation and multiplexing the carry output, instead of generating the sum outputs. It should be noted that unlike other stages, the least significant stage of the CSA is only built by one RCA without multiplexers, since it is directly driven by the carry input. Hence we can obtain the delay of the $i^{th}$ stage as presented in~(\ref{CSA_SingleStageDelay}), where $\mu_{c}$, $\mu_{mux}$ and $n_i$ denote the delay of $1$-bit carry propagation, the delay of multiplexing and the word-length of the $i^{th}$ stage of the CSA, respectively.
\begin{eqnarray}\label{CSA_SingleStageDelay}
  d_i=\left\{
    \begin{matrix}
      n_i\cdot \mu_{c}+(s-i)\cdot\mu_{mux}, &\textrm{if $i\in\left[1,s-1\right]$}\\
      n_0\cdot \mu_c + (s-1)\cdot\mu_{mux}, & \textrm{if $i=0$}
    \end{matrix}
  \right.
\end{eqnarray}

Under the timing-driven design environment, the delay of each stage of CSA is equalized in order to achieve the fastest operation, as presented in~(\ref{CSA_DelayEqual}).
\begin{eqnarray}\label{CSA_DelayEqual}
  d_{s-1}=d_{s-2}=\cdots=d_{0}
\end{eqnarray}

In this case, combining~(\ref{CSA_SingleStageDelay}) and (\ref{CSA_DelayEqual}) yields $n_i$, which is represented by the word-length of the most significant stage $n_{s-1}$, in (\ref{CSA_DelayRep}).
%This design metric also consumes the maximum word-length of CSA for a given frequency, potentially corresponds to the minimum precision loss at inputs. Substituting (\ref{CSA_SingleStageDelay}) into (\ref{CSA_DelayEqual}) yields (\ref{CSA_DelayRep}), where $n_i$ denotes the CSA word-length of stage $i$.
\begin{eqnarray}\label{CSA_DelayRep}
 n_i=\left\{
	\begin{matrix}
	  n_{s-1}-(s-1-i)\cdot\frac{\mu_{mux}}{\mu_c}, & \textrm{if $i\in\left[1,s-1\right]$}\\
	  n_{s-1}-(s-2)\cdot\frac{\mu_{mux}}{\mu_c}, &\textrm{if $i=0$}
	\end{matrix}
    \right.
\end{eqnarray}

Sum up $n_i$ to give the total word-length of the CSA in (\ref{CSA_WL}).
%The total word-length of the CSA is the sum of every stage, as given by (\ref{CSA_WL}).
\begin{eqnarray}\label{CSA_WL}
  \begin{split}
    n_{CSA} &=\sum_{i=0}^{s-1}n_{i} \\
    &= s\cdot n_{s-1}-\frac{\mu_{mux}}{\mu_{c}}\cdot\frac{(s+1)(s-2)}{2}
  \end{split}
\end{eqnarray}

Conventionally under a given frequency constraint, the word-length of RCA is truncated by using $n_{RCA}=b-1$ bits, where $b$ is determined by~(\ref{Max.length of carry chain}). Similarly for CSA, the word-length of each stage should be selected in order to satisfy~(\ref{TimingConstraint_CSA}).
\begin{eqnarray}\label{TimingConstraint_CSA}
  \begin{matrix}
    \forall i\in[0,s-1] &, & d_i\leqslant\frac{1}{f_s}
  \end{matrix}
\end{eqnarray}

Hence for the same frequency requirement, we can form the relationship between the delay of the most significant stage of CSA and the delay of RCA, as given by~(\ref{WL_MSB_CSA}),
\begin{eqnarray}\label{WL_MSB_CSA}
    \mu_c\cdot (b-1) = n_{s-1}\cdot\mu_c+\mu_{mux}
\end{eqnarray}

%, the word-lengths of RCAs and CSAs are selected to avoid timing violation, as presented in~(\ref{CSA_RCA}).

Substitute~(\ref{WL_MSB_CSA}) into (\ref{CSA_WL}) to replace $n_{s-1}$, we derive the representation of the word-length of CSA in terms of $b$, as presented in~(\ref{CSA_Timing}).
% we form the relationship between the word-length of CSA and RCA under a given timing constraint, as presented in~(\ref{CSA_Timing}).
\begin{eqnarray}\label{CSA_Timing}
  n_{CSA}=s\cdot (b-1)-\frac{\mu_{mux}}{\mu_{c}}\cdot\frac{(s+2)(s-1)}{2}
\end{eqnarray}

%It can be seen that $s=1$ leads to $n_{CSA}=n_{RCA}$. This is because RCA forms the least significant stage of CSA. In addition, combine (\ref{CSA_DelayRep}) and (\ref{CSA_WL}) to ensure $n_{s-1}>0$, we obtain the upper bond of the stage number by~(\ref{CSA_StageNumber}).
%\begin{eqnarray}\label{CSA_StageNumber}
%  s<n_{RCA}\cdot\frac{\mu_c}{\mu_{mux}}+1
%\end{eqnarray}


%\subsection{Model Verification}
%Using this information, we verify our timing model with experimental results, which are obtained from post place and route simulations on Xilinx Virtex-6 FPGAs. Fig.~\ref{CSA Model Verification} illustrates both the modeled value and the experimental results of the maximum word-length of the 2-stage CSA for various operating frequencies. In the following experiments within this section, the input data are randomly sampled from a 16-bit data set, which follows uniform distribution. It can be seen that the the modeled value match well with the experimental results.

%
%\begin{figure}[thbp]
%  \centering
%  \includegraphics[width=3in]{./Figures/Model.eps}
%  \caption{Comparison of the maximum word-length between the modeled value and the experimental results from FPGA simulations.}
%  \label{CSA Model Verification}
%\end{figure}

\subsection{Accuracy Benefits and Area Overhead in CSA}
We first verify the models for the RCA and CSA in terms of the maximum word-length under the given operating frequencies. For the CSA, the ratio $\mu_{mux}/\mu_c$ can be computed experimentally. We perform post place-and-route simulations on the CSA with 2 stages using Xilinx Virtex-6 FPGA. The delay of the $i^{th}$ stage $d_i$ in~(\ref{CSA_SingleStageDelay}) is recorded with respect to different word-lengths. The total word-length of CSA can be predicted through~(\ref{CSA_Timing}). In addition, the maximum word-lengths of the 2-stage CSA and RCA are obtained experimentally by increasing $n_{CSA}$ and $n_{RCA}$ respectively until errors are observed at the output. The comparison between the modeled value and the empirical results is illustrated in Fig.~\ref{Fig_CSA2stage_Timing}. It can be seen that our models for both RCA and CSA match well with the experimental results.

\begin{figure}[tbp]
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \subfigure[The modeled value and the experimental results of the maximum word-length of RCA and CSA.]{
            \centering
            \includegraphics[width=0.95\textwidth]{./Figures/CSA2stage_Model_Timing.eps}
            \label{Fig_CSA2stage_Timing}
        }
        \subfigure[Hardware resource usage for an RCA and a CSA with 2 and 3 stages.]{
            \centering
            \includegraphics[width=0.95\textwidth]{./Figures/CSA3stage_Area.eps}
            \label{Fig_CSA3stage_Area}
        }
    \end{minipage}
    \centering
    \caption{A comparison between RCA and CSA in terms of the maximum word-length of input signal and the area consumption.}
\end{figure}

%As greater word-length corresponds to smaller truncation errors, we compare the maximum word-length of RCA and CSA with 2 stages and 3 stages over a range of operating frequencies, as presented in

Fig.~\ref{Fig_CSA2stage_Timing} also highlights that in comparison to the RCA, the CSA achieves greater word-length when frequency is initially increased. The RCA only outperforms than the CSA when very high frequency is applied. This is because at low frequencies, although the multiplexer delay limits the word-length of each stage in CSA when compared to RCA, the stage parallelism in CSA enables a greater word-length. However when frequency increases, the multiplexer delay becomes comparable to the delay of the carry chain, and this inhibits the benefits of parallelism.
%In addition, we can see that the word-length of 3-stage CSA is always greater than 2-stage CSA across the entire frequency domain.


However, the accuracy benefits brought by CSA comes at the cost of a large area overhead. Fig.~\ref{Fig_CSA3stage_Area} depicts the number of Look-Up Tables (LUTs) in the FPGA used for an RCA, a 2-stage CSA and a 3-stage CSA. It can be seen that in order to meet a given frequency, the 3-stage CSA consumes $2.4\times\sim3.7\times$ area than RCA, while the 2-stage CSA requires $1.7\times\sim3.1\times$ extra area. This finding poses a question of which is the best adder structure for a specific area budget.

\section{Choosing the Optimum Adder Structure}\label{Section_OptimumAdder}
In Section~\ref{section_RCA}, we discussed two design scenarios for the RCA when considering timing constraints. In this section, we expand our analysis by incorporating silicon area as another evaluation metric, and investigate the accuracy, performance and area trade-offs for different adder structures. In the conventional design scenario, the word-length of RCA and CSA is limited by the given frequency constraint, or/and the available hardware resources. The precision loss potentially generates large errors even without timing violations. However in the new design scenario, we use the RCA with the maximum possible word-length under the given area budget, and the timing constraints are allowed to be violated. This process might result in timing errors as well as truncation errors due to area limitation. We compare these two scenarios with different design goals with the aim of finding the optimum design methodology under each situation.

%To answer this question, the silicon area is incorporated into the trade-offs together with accuracy and performance. If the available hardware resources are limited, the full word-length of both CSA and RCA might not be implemented. The precision loss might results in large errors even at low frequencies. This is demonstrated by experiments where area constraints are applied besides the timing requirement. In the conventional design scenario, both of the constraints are met by reducing the word-length of the input signal. The errors at the outputs are recorded with the reference to the original word-length, which is 16 bits in the following experiments. In addition, we propose another circuit designing method where RCA is implemented with the maximum possible word-length under the given area budget, while the timing constraints are met by overclocking.

%Our results could be of interest to a circuit designer in two aspects. On one hand, it is desirable to create a circuit that can run at a given frequency with the minimum achievable output errors and resource usage. On the other hand, the algorithm designer will wish the circuit to operate as fast as possible with the minimum area whilst a certain error budget can be tolerated. We select the optimum design methodology for both situations.
\subsection{Determination of the Optimum Adder Structure for Given Frequency Requirements}
First, suppose the algorithm designer wished to create a circuit that can run at a given frequency with the minimum achievable output errors and minimum resource usage, i.e. a pair of $\{Area, Frequency\}$ constraint is applied. In this case, the optimum design method is selected according to the following criteria:
\begin{itemize}
  \item Design with the minimum mean value of errors at the output is the optimum design;
  \item If multiple designs achieve the same accuracy, then the design with minimum area is the optimum design;
  \item If both the accuracy and area are identical for multiple designs, they are all treated as the optimum design.
\end{itemize}

For instance, in Fig.~\ref{Fig_CSA_LUT25} we record the mean value of error at outputs with respect to different operating frequencies for both design scenarios when the number of the available LUTs is set to 25. In this graph, we have labeled the optimum adder structure. Note that in this and all the following experiments within this section, in order to apply our models the input data is randomly generated following the uniform distribution.

%, the input data are randomly generated following uniform distribution, in order to keep consistence with our models.

\begin{figure}[th]
  \centering
  \includegraphics[width=3.3in]{./Figures/Error_LUT25.eps}
  \caption{A comparison between two design scenarios when the number of available LUT is 25. The RCA and CSA are investigated in the conventional scenario, while the RCA is explored in the proposed new scenario. The results are obtained from post place-and-route simulations on Xilinx Virtex-6 FPGAs.}
  \label{Fig_CSA_LUT25}
\end{figure}

We first notice that for all frequency values, the overclocked RCA achieves smaller error expectation than the RCA with truncated operand word-lengths, as predicted by the models for the RCA in Section~\ref{section_RCA_Comparison}. It can also be observed that the CSA cannot be implemented with the original word-length due to the limited area budget and this leads to large truncation errors. Although the CSA with 3 stages outperforms temporarily when frequency is further increased, the overclocked RCA is still the optimum design when high operating frequencies are applied.

We then perform similar experiments with a variety of area constraints. The optimum design methods with respect to different operating frequencies and area consumptions are demonstrated in Fig.~\ref{Fig_CSA_Tradeoff}. From this figure, several observations can be made. Firstly, if the available area is large enough to implement a CSA in full precision, it will be the optimum design. This is expected from our earlier analysis in Fig.~\ref{Fig_CSA2stage_Timing}. Secondly, the 2-stage CSA is better than the 3-stage CSA when frequency is initially increased, as it consumes less area although both achieve the same error expectation. Thirdly, only part of the CSA can be implemented under a tighter area budget, whereas the RCA still keeps full precision. In this case, area becomes the dominate factor and precision is lost for the CSA, meaning the RCA with overclocking is the optimum design method across almost the whole frequency domain in this situation. Last but not the least, for a more stringent area constraint, the word-length of RCA is also limited. This results in truncation errors initially for all design scenarios. However, the RCA with overclocking can still be employed as the optimum design, because it loses less precision than the CSA under the same area constraint.

\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in]{./Figures/Tradeoff.eps}
  \caption{A demonstration of the optimum design methodology which achieves the minimum error at outputs with respect to a variety of frequency and area constraints.}
  \label{Fig_CSA_Tradeoff}
\end{figure}

\subsection{Determination of the Optimum Adder Structure for Given Accuracy Requirements}
If the design goal is to operate the circuit as fast as possible with the minimum area whilst a certain error budget can be tolerated, the optimum design methodology can be decided as illustrated in Fig.~\ref{Fig_CSA_Tradeoff_Error}. In this situation, the error specifications are evaluated in terms of the mean relative error (MRE), as given by~(\ref{MRE}), where $E_{error}$ and $E_{out}$ refer to the mean value of error and the mean value of outputs, respectively.
\begin{eqnarray}\label{MRE}
  MRE=\left|\frac{E_{error}}{E_{out}}\right|\times 100\%
\end{eqnarray}

In our experiments, MRE is set ranging from $0.001\%$ to $50\%$. For a certain value of MRE, the design with the maximum operating frequency is selected as the optimum design. Moreover, the smallest design is the optimum one if multiple structures operate at the same frequency, with a certain area requirement. Fig.~\ref{Fig_CSA_Tradeoff_Error} can thus be obtained based on these criteria.
%Based on these criteria, the decision graph is depicted in
%We also notice that for small frequencies, RCA with either overclocking or truncation of word-length of the input signal is the best design choice for any area constraint, since
\begin{figure}[t]
    \centering
    \includegraphics[width=3.7in]{./Figures/Tradeoff_Error.eps}
    \caption{A demonstration of the optimum design methodology which runs at the fastest frequency with respect to a variety of accuracy and area constraints.}
    \label{Fig_CSA_Tradeoff_Error}
\end{figure}

For a tight accuracy requirement, i.e. $MRE<0.005\%$, CSA serves as the optimum design choice with respect to large accessible area, as it intrinsically operates faster than RCA. Once again, when the area budget shrinks, the RCA performs best because the precision of the CSA is limited. Similarly to the results in Fig.~\ref{Fig_CSA_Tradeoff}, we see that the overclocked RCA achieves the fastest operating frequencies under most area constraints when the accuracy requirement is released.

\subsection{Design Guidance}
To sum up, the experiments reveal that for both design goals, CSA is the best option only when there is enough hardware resources. However for the remainder of this paper, we are interested in cases where there is a limited area budget, as may be the case in the embedded applications. Due to this reason, we will focus on the RCA in our following analysis and experiments.

%this circumstance is unlikely to happen in the practice, especially with the continued technology scaling nowadays. Due to this reason, in the following sections we will focus on the situation where the hardware resources are limited, and therefore RCA is the design option in our following analysis and experiments.

\section{Constant Coefficient Multiplier}\label{section_CCM}

As another key primitive of arithmetic operations, CCM can be implemented using RCA and shifters. For example, operation $B=9A$ is equivalent to $B=A+8A=A+(A<<3)$, which can be built using one RCA and one shifter. We first focus on a single RCA and single shifter structure. We describe how more complex structures consisting of multiple RCAs and multiple shifters can be built in accordance with this baseline structure in Section~\ref{CCM_Multi}.

In this CCM structure, let the two inputs of the RCA be denoted by $A_S$ and $A_O$ respectively, which are both two's complement numbers. $A_S$ denotes the ``shifted signal", with zeros padded after LSB, while $A_O$ denotes the ``original signal" with MSB sign extension. For an $n$-bit input signal, it should be noted that an $n$-bit RCA is sufficient for this operation, because no carry will be generated or propagated when adding with zeros, as shown in Fig.~\ref{CCM_fig}.
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{./Figures/CCM_CarryChain.pdf}
  \caption{Four possible carry chain types in a constant coefficient multiplier with $n$-bit inputs. The notion $s$ denotes the shifted bits and $BP$ denotes the binary point.}
  \label{CCM_fig}
\end{figure*}

\subsection{Probabilistic Model of Truncation Error}
Let $E_{Tin}$ and $E_{Tout}$ denote the expectation of truncation error at the input and output of CCM respectively. We then have (\ref{Et_CCM}), where $coe$ denotes the coefficient value of the CCM, and $E_{Tin}$ can be obtained according to~(\ref{TruncationError}).
%
\begin{eqnarray}\label{Et_CCM}
%\vspace{-1ex}
%\small
%\footnotesize
%\scriptsize
  E_{Tout}=|coe|\cdot E_{Tin}
%  \vspace{-1ex}
\end{eqnarray}
%\normalsize

\subsection{Probabilistic Model of Overclocking Error}
\subsubsection{Absolute Value of Overclocking Error}
The absolute value of overclocking error of carry chain $C_{tm}$ is increased by a factor of $2^s$ due to shifting, compared to RCA. Hence $e_{tm}$ in CCM can be modified from (\ref{etm}) to give (\ref{etm_CCM}).
%
\begin{eqnarray}\label{etm_CCM}
%\vspace{-1ex}
%\small
%\footnotesize
  e_{tm}=2^{t+b-n+s}
%  \vspace{-1.5ex}
\end{eqnarray}
%\normalsize

\subsubsection{Probability of Overclocking Error}
Due to the dependencies in a CCM, carry generation requires $a_t=a_{t-s}=1$, propagation and annihilation of a carry chain is best considered separately for four types of carry chain generated at bit~$t$. We label these by $C_{tm}1$ to $C_{tm}4$ in Figure~\ref{CCM_fig}, defined by the end region of the carry chain. For $C_{tm}1$, we have:
\begin{itemize}
  \item Carry propagation: $\!a_i\neq\!a_{i+s}\!$ where $\!i\!\in[t+1,n-s-2]\!$;
  \item Carry annihilation: $\!a_j\!=\!a_{j+s}\!$ where $\!j\!\in[t+1,n-s-1]\!$.
\end{itemize}
Similarly for $C_{tm}2$, we have:
\begin{itemize}
  \item Carry propagation: $a_i\neq a_{n-1}$ where $i\in[n-s-1,n-3]$; or $a_i\neq a_{i+s}$ where $i\in[t+1,n-s-2]$;
  \item Carry annihilation: $\!a_j\!=a_{n-1}\!$ where $j\!\in\![n-s-1,n-2]$.
\end{itemize}

For the first two types of carry chain $C_{tm}1$ and $C_{tm}2$, the probability of carry propagation and annihilation is $1/2$ and the probability of carry generation is $1/4$, under the premise that all bits of input signal are mutually independent. Therefore (\ref{Ptm_CCM1}) can be obtained by substituting this into~(\ref{Ptm_RCA}).
%
\begin{eqnarray}\label{Ptm_CCM1}
%\vspace{-1ex}
%\small
%\footnotesize
  P_{tm}=\left(1/2\right)^{m+1}, & \textrm{if $t+m-1\leqslant n-2$}
\end{eqnarray}
%\normalsize

For carry annihilation of $C_{tm}3$, $a_{n-1}=a_{n-1}$, which is always true. Thus the probability of $C_{tm}3$ is given by (\ref{Ptm_CCM2}).
%
\begin{eqnarray}\label{Ptm_CCM2}
%\vspace{-1ex}
%\small
%\footnotesize
  P_{tm}=\left(1/2\right)^m, & \textrm{if $t+m-1=n-1$}
%  \vspace{-1ex}
\end{eqnarray}
%\normalsize
$C_{tm}4$ represents carry chain annihilates over $a_{n-1}$, therefore carry propagation requires $a_{n-1}\neq a_{n-1}$. This means $C_{tm}4$ never occurs in a CCM.

Altogether, $P_{tm}$ for a CCM is given by (\ref{Ptm_CCM}).
\begin{eqnarray}\label{Ptm_CCM}
%\small
  P_{tm}=\left\{\begin{array}{ll}
      (1/2)^{m+1} & \textrm{if $t+m-1<n-1$}\\
      (1/2)^{m} & \textrm{if $t+m-1=n-1$}
    \end{array} \right.
    %\vspace{-1ex}
\end{eqnarray}
%\normalsize

\subsubsection{Expectation of Overclocking Error}
Since the carry chain of a CCM will not propagate over $a_{n-1}$, the upper bound of parameter $t$ and $m$ should be modified from (\ref{t_RCA}) and (\ref{m_RCA}) to give (\ref{t_CCM}) and (\ref{m_CCM}).
%
\begin{eqnarray}
%\vspace{-1ex}
%\small
%\footnotesize
%\scriptsize
  \label{t_CCM} 0\leqslant t\leqslant n-b-1\\
  \label{m_CCM} b<m\leqslant n-t
%  \vspace{-1ex}
\end{eqnarray}
%\normalsize

Finally, by substituting (\ref{Ptm_CCM}) and (\ref{etm_CCM}) with modified bounds of $t$ and $m$ into (\ref{Eo_exp}), we obtain the expectation of overclocking error for a CCM to be given by (\ref{Eo_CCM}).
%
\begin{eqnarray}\label{Eo_CCM}
%\vspace{-1ex}
%\small
%\footnotesize
      E_O=\left\{
        \begin{matrix}
            2^{s-b-1}-2^{s-n-1}, & \textrm{if $b\leq n-1$}\\
            0, & \textrm{otherwise}
        \end{matrix}
      \right.
%      \vspace{1ex}
\end{eqnarray}
%\normalsize

\subsection{CCM with Multiple RCAs and Shifters}\label{CCM_Multi}
In the case where a CCM is composed of two shifters and one RCA, such as operation $\!B\!=\!20A\!=\!(\!A<<2)+(\!A<<4)$, let the shifted bits be denoted as $s_1$ and $s_2$ respectively. Hence the equivalent $s$ in (\ref{Eo_CCM}) can be obtained through~(\ref{ShiftNo}).
%
%\vspace{-3ex}
\begin{eqnarray}\label{ShiftNo}
%\footnotesize
%\scriptsize
  s=\left|s_1-s_2\right|
%  \vspace{-1ex}
\end{eqnarray}

For those operations such as $B=37A=(A<<5)+(A<<2)+(A<<1)$, the CCM can be built using a tree structure. Each root node is the baseline CCM and the errors are propagated through an adder tree, of which the error can be determined based on our previous RCA model.

\section{Test Platform}\label{Section_ExpSetup}
In our experiments, we compare two design perspectives. In the first scenario, the word-length of the input signal is truncated before propagating through the datapath in order to meet a given latency. In our proposed overclocking scenario, the circuit is overclocked while keeping the original operand word-length. The benefits of the proposed methodology are demonstrated over a set of DSP example designs, which are implemented on the Xilinx ML605 board with a Virtex-6 FPGA.
%XC6VLX240T-1FFG1156.

\subsection{Experimental Setup} \label{sub:experimental_setup}
We initially build up a test framework on an FPGA. The general architecture is depicted in Fig.~\ref{Test Framework}. The main body of the test framework consists of the circuit under test (CUT), the test frequency generator and the control logic, as shown in the dotted box in Fig.~\ref{Test Framework}. The I/Os of the CUT are registered by the launch registers (LRs) and the sample registers (SRs), which are all triggered by the test clock. Input test vectors are stored in the on-chip memory during initialization. The results are sampled using Xilinx ChipScope. Finally, we perform an offline comparison of the output of the original circuit at the rated frequency with the output of the overclocked as well as the truncated designs using the same input vectors.
%
\begin{figure}[htbp]
  \centering
  %\vspace{-3.5ex}
  \includegraphics[width=3.5in]{./Figures/TestFramework.pdf}
  %\vspace{-4.5ex}
  \caption{Test framework, which is composed of a measurement architecture (the dotted box) on an FPGA and an off-line comparator using software. Note that the error-free data are obtained by either pre-computation or initial run with low frequencies.}
  \label{Test Framework}
  %\vspace{-1ex}
\end{figure}

The test frequency generator is implemented using two cascaded mixed-mode clock managers (MMCMs), created using Xilinx Core Generator~\cite{Virtex6Clocking}. Besides the outputs, the corresponding input vectors and memory addresses are also recorded into the comparator, as can be seen in Fig.~\ref{Test Framework}, in order to ensure that the recorded errors arise from overclocking the CUT rather than the surrounding circuitry when high test frequencies are applied.

%the potential timing errors generated from the test framework instead of CUT, when high test frequencies are applied. Comparisons are performed between the sampled results and the expected values. If mismatch is found, the results will be discarded.

%. In this test framework, two cascaded MMCMs are employed in order to provide a wide range of frequency values. In addition, the output frequency of the MMCM can be dynamically reconfigured without re-implementing the whole design~\cite{Virtex6Clocking,MMCM}. This reconfiguration process is controlled by a finite state machine within the control logic. Before actual measurements on FPGA, the rated frequency of CUT is examined through Xilinx Timing Analyzer. This frequency is then utilized as the starting value when generating test frequencies. The new frequency will increase with a step of 10MHz and stop when the comparator shows that output errors are stabilized. The control logic also generates address and read enable signal for the on-chip memory.

%In our experiments, the test clock frequency starts from the rated operating frequency of CUT, and increases with a step of 10MHz until the output error expectation stabilizes.

%In order to avoid the potential timing errors generated from the test framework instead of CUT under high test frequencies, several methods are adopted. Firstly, the control signals for the test frequency generator is triggered by the system clock, which is much lower than the test frequency. This will guarantee a correct generation of the test clock. Secondly, since the address counter for the on-chip memory is driven by the test frequency, a deeply pipelined counter structure with initial delay of 258 cycles is used to ensure the correct sample of each address bit. Thirdly, the key internal control signals such as memory address and corresponding operands are recorded together with the outputs to the comparator. Comparisons are performed between these values and the corresponding original values. The results will be discarded if mismatch is found.

\subsection{Benchmark Circuits}
Three types of DSP designs are tested: digital filters (FIR, IIR and Butterworth), a Sobel edge detector and a direct implementation of a Discrete Cosine Transformation (DCT). The filter parameters are generated through MATLAB filter design toolbox, and they are normalized to integers for implementation. Table~\ref{Rated_Frequency_table} summarizes the operating frequency of each implemented design in Xilinx ISE14.1 when the word-length of input signal is 8-bit.
\vspace{-0.5ex}
\begin{table}[htbp]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.2}
%\setlength{\tabcolsep}{2}
\caption{Rated Frequencies of Example Designs.}
\label{Rated_Frequency_table}
\begin{center}
\footnotesize
%\vspace{-4ex}
\begin{tabular}{ccc}
\hline
\hline
Design & Frequency (MHz) & Description\\
\hline
FIR Filter & 126.2 & $5^{th}$ order\\
Sobel Edge Detector & 196.7 & $3\times 3$\\
IIR Filter & 140.3 & $7^{th}$ order\\
Butterworth Filter & 117.1 & $9^{th}$ order\\
DCT & 176.7 & 4-point\\
\hline
\hline
\end{tabular}
\normalsize
\end{center}
%\vspace{-2.5ex}
\end{table}

The input data are generated from two sources. One is called ``uniform independent inputs'', which are randomly sampled from a uniform distribution of 8-bit numbers. The other is referred to as ``real inputs'', which denote 8-bit pixel values of the $512\! \times\! 512$ Lena image.

\subsection{Exploring the Conservative Timing Margin} % (fold)
\label{sub:exploring_the_conservative_timing_margin}

Generally, the operating frequency provided by EDA tools tends to be conservative to ensure the correct functionality under a wide range of operating environments and workloads. In a practical situation, this may result in a large gap between the predicted frequency and the actual frequency under which the correct operation is maintained~\cite{gojman2013FPGA}.

For example, the predicted frequencies and the actual frequencies of a $5^{th}$~order FIR filter using different word-lengths are depicted in Fig.~\ref{TimingMargin}. The ``actual'' maximum frequencies are computed by increasing the operating frequency from the rated value until errors are observed at the output; the maximum operating frequency with correct output is recorded for the current word-length. As can be seen in Fig.~\ref{TimingMargin}, the circuit can operate without errors at a much higher frequency in practice than predicted according to our experiments. A maximum speed differential of $3.2\times$ is obtained when the input signal is 5-bit.

\begin{figure}[t]
  \centering
  \includegraphics[width=3.2in]{./Figures/Exp/FIR/RatedFrequency3.eps}
  %\vspace{-1.5ex}
  \caption{The maximum operating frequencies for different input word-lengths of an FIR filter. The dotted line depicts the rated frequency reported by the timing analysis tool. The solid line is obtained through real FPGA tests using our platform.}
  \label{TimingMargin}
  %\vspace{-3.5ex}
\end{figure}

In our experiments in Section~\ref{Section_Experiments}, the conservative timing margin is removed in the traditional scenario for a fairer comparison to the overclocking scenario. To do this, for each truncated word-length, we select the maximum frequency at which we see no overclocking error on the FPGA board in our lab. For example, in Fig.~\ref{TimingMargin}, the operating frequency of the design when the word-lengths are truncated to 8,~5 and 2 bits are 400MHz,~450MHz and 500MHz respectively.

Fig.~\ref{TimingMargin} also demonstrates that when the circuit is truncated, it allows the circuit to operate at a higher frequency than the frequency of full precision implementation. However, a non-uniform period change can be observed for both results. For instance, the maximum operating frequency keeps almost constant when the operand word-length reduces from 8 to 6 or from 5 to 3 in both the experimental results and those of timing analyzer. This will cause a slight deviation between our analytical model which assumes that the single bit carry propagation delay to be a constant value, as discussed in (\ref{Eo_trad_RCA}) with expression $n=b-1$. This deviation will be influenced by many factors including how the architecture has been packed onto LUTs and CLBs and process variation causing non-uniform interconnection delays~\cite{FPGAPV}. However, we shall see that our model remains close to the true empirical results in Section~\ref{Section_Experiments}.



%\subsection{Evaluation Metric of Outputs}
%The results are evaluated in terms of mean relative error (MRE), which represents the percentage of error at outputs. MRE is given by~(\ref{MRE}), where $E_{error}$ and $E_{out}$ refer to the mean value of error and the correct output respectively.
%%
%\begin{eqnarray}\label{MRE}
%%\small
%\scriptsize
%  MRE=\left|\frac{E_{error}}{E_{out}}\right|\times 100\%
%\end{eqnarray}
%\normalsize

\subsection{Computing Model Parameters}
The accuracy of our proposed models is examined with practical results on Virtex-6 FPGA. We first determine the model parameters. There are two types of parameters in the models of overclocking error. The first is based on the circuit architecture. For example, the word-length of RCAs and CCMs ($n$), the shifted bits of the shifters in CCM ($s$), and the word-length of the input signal ($k$). This is determined through static analysis. The second depends on timing information, such as the single bit carry propagation delay $\mu$. In order to keep consistency with the assumption made in models that $\mu$ is a fixed value, it is obtained according to the actual FPGA measurement results.

Initially the maximum error-free frequency $f_0$ is applied. In this case we have~(\ref{Eq_decideMu_0}) where $d_c$ is a constant value which denotes the interconnection delay, and $d_0=1/f_0$. The frequency is then increased such that~(\ref{Eq_decideMu_1}) is obtained. This process repeats until the maximum frequency $f_{n-1}$ is applied in~(\ref{Eq_decideMu_2}). Based on these frequency values, $\mu$ can be determined.

\begin{eqnarray}\label{Eq_decideMu_0}
  %\frac{1}{f_0} &= n\mu+d_c
  d_0 &= n\mu+d_c
  \vspace{-13ex}
\end{eqnarray}
\vspace{-4ex}
\begin{align}\label{Eq_decideMu_1}
  d_1 &= (n-1)\mu+d_c    \\
  &\cdots               \notag
  \vspace{-3ex}
\end{align}
\vspace{-4ex}
\begin{eqnarray}\label{Eq_decideMu_2}
  {d_{n-1}} &= \mu+d_c
\end{eqnarray}

\section{Results and Discussion}\label{Section_Experiments}
\subsection{Case study: FIR filter}
We first assess the accuracy of our proposed models of error. The modeled values of both overclocking error and truncation error of the FIR filter are presented in Fig.~\ref{FIR} (dotted lines), as well as the actual measurements on the FPGA (solid lines) with two types of input data. The results demonstrate that our models match well with the practical results obtained using the uniform independent inputs.
% FIR
\begin{figure}[tbp]
  \centering
%  \vspace{-2.5ex}
  \includegraphics[width=3.5in]{./Figures/Exp/FIR/FIR_Error_New_TCAD.eps}
%  \vspace{-5ex}
  \caption{A demonstration of two design perspectives with a $5^{th}$~order FIR filter, which is implemented on Virtex-6 FPGA. The modeled values of both overclocking errors and truncation errors are presented as dotted lines. The actual FPGA measurements are depicted using solid lines. Two types of inputs are employed in the overclocking scenario: the uniformly distributed data and the real image data from Lena.}
  \label{FIR}
\end{figure}

%Lena image
\begin{figure*}[htbp]
  %\vspace{-2ex}
  \centering
  \subfigure[425MHz,~n=8~,~SNR=47.97dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_T8.eps}
  \end{minipage}%
  }% This is important! use % to indicate same line, otherwise new line
  \subfigure[450MHz,~n=8,~SNR=44.94dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_T65.eps}
  \end{minipage}
  }%
  \subfigure[480MHz,~n=8,~SNR=21.58dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_T62.eps}
  \end{minipage}
  }%
  \subfigure[520MHz,~n=8,~SNR=10.03dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_T6.eps}
  \end{minipage}
  }
  \subfigure[425MHz,~n=7,~SNR=26.02dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_Q7T10.eps}
  \end{minipage}%
  }% This is important! use % to indicate same line, otherwise new line
  \subfigure[450MHz,~n=5,~SNR=21.76dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_Q4T10.eps}
  \end{minipage}
  }%
  \subfigure[480MHz,~n=2,~SNR=6.57dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_Q2T10.eps}
  \end{minipage}
  }%
  \subfigure[520MHz,~n=1,~SNR=3.95dB]{
  \begin{minipage}[c]{0.24\textwidth}
    \centering
    \includegraphics[width=1.8in]{./Figures/Exp/FIR/LenaImages/FIRLena_Q1T10.eps}
  \end{minipage}
  }
%\vspace{-2ex}
\caption{Output images of the FIR filter for both overclocking scenario (top row) and traditional scenario (bottom row) under various operating frequencies.}
\label{FIR_Lena}
\end{figure*}

According to Fig.~\ref{FIR}, output errors are reduced in the overclocking scenario for both input types in comparison to the traditional scenario, as the analytical model validates. In addition, we see that using real data, more significant reduction of MRE are achieved, and that no errors are observed when frequency is initially increased. This is because for real data, long carry chains are typically generated with even smaller probabilities, and the longest carry chain rarely occurs.

The output images of the FIR filter for both of the two scenarios with increasing frequencies are presented in Fig.~\ref{FIR_Lena}, from which we can clearly see the differences between the errors generated in these two scenarios. In the overclocking scenario, we observe errors in the MSBs for certain input patterns. This leads to ``salt and pepper noise'', as shown on the images in the top row of Fig.~\ref{FIR_Lena}. In the traditional scenario, truncation causes an overall degradation of the whole image, as can be seen in the bottom row of Fig.~\ref{FIR_Lena}. Furthermore, it is difficult to recover from the latter type of error, since it is generated due to precision loss.

In addition, we record the probability distribution of different length of carry chains using an 8-bit RCA with both data types, as shown in Fig.xxx. As the input data is 8-bit, the longest possible carry chain length is 9-bit. It can be observed that when using Lena data,  the probability is only higher for carry chain with a length of 3-bit. While in other situations, using uniform data leads to higher probability, especially for long carry chain length. This finding explicitly demonstrates that longer carry chains happens with an even smaller probability when utilizing real data.

% for short carry chains (i.e. carry chain length of 2-bit or 3-bit),

%In detail, the difference between the two types of data is explicitly

\begin{figure}[tbp]
  \centering
  \includegraphics[width=3.5in]{./Figures/CC_length2.eps}
  \caption{Probability distribution of different length of carry chains in a 8-bit RCA. For the uniform data, two inputs of RCA are randomly sampled from a uniform distribution. For the image data Lena, one input of RCA uses the original data and the other uses the delayed original data for several clock cycles.}
  \label{CarryDistribution}
\end{figure}

%\begin{figure}[tbp]
%    \begin{minipage}[b]{0.48\textwidth}
%        \centering
%        \subfigure[The cumulative distribution function with respect to both type of input data.]{
%            \centering
%            \includegraphics[width=0.95\textwidth]{./Figures/CDF.eps}
%            \label{Fig_CSA2stage_Timing}
%        }
%        \subfigure[Probability distribution of different length of carry chains in the $5^{th}$ order FIR filter, using both type of input data.]{
%            \centering
%            \includegraphics[width=0.95\textwidth]{./Figures/CC_length2.eps}
%            \label{Fig_CSA3stage_Area}
%        }
%    \end{minipage}
%    \centering
%    \caption{Statistical analysis of two types of input data: the uniform data and the real image data: Lena.}
%\end{figure}


\subsection{Potential Benefits in Datapath Design}
%The results can be interpreted in two folds.
As we mentioned in Section~\ref{Section_OptimumAdder}, our results could be of interest to a circuit designer in two ways. Typically, either the designer will want to create a circuit that can run at a given frequency with the minimum possible MRE, or the algorithm designer will wish to run as fast as possible whilst maintaining a specific error tolerance. For the first design target, the experimental results for all five example designs on FPGA are summarized in Table~\ref{Ratio_MRE} in terms of the relative reduction of MRE as given in~(\ref{Relative Reduction MRE}) where $MRE_{Trad}$ and $MRE_{ovrc}$ denote the value obtained in the traditional scenario and in the overclocking scenario, respectively.

\begin{eqnarray}\label{Relative Reduction MRE}
  \frac{MRE_{Trad}-MRE_{ovrc}}{MRE_{Trad}}\times 100\%
\end{eqnarray}

In this table, the frequency is normalized to the maximum error-free frequency for each design when the input signal is 8-bit. The N/A in Table~\ref{Ratio_MRE} refers to the situations where a certain frequency simply cannot be achieved using the traditional scenario. It can be seen that a significant reduction of MRE can be achieved using the proposed overclocking scenario, and the geometric mean reduction varies from $67.9\%$ to $95.4\%$ using uniform input data. Even larger differences of MRE can be observed when testing with real image data for each design, ranging from $83.6\%$ to $98.8\%$, as expected given the results shown in Fig.~\ref{FIR}.
%Table goes here

\begin{table*}[htbp]
 %\vspace{-2ex}
 %% increase table row spacing, adjust to taste
   \renewcommand{\arraystretch}{1.05}
   \setlength{\tabcolsep}{3.5pt}
   \caption{Relative Reduction of MRE in Overclocking Scenario for Various Normalized Frequencies Based on~(\ref{Relative Reduction MRE}).}
%   \vspace{-2ex}
   \centering
   \footnotesize
   %\vspace{-4mm}
   \begin{threeparttable}
   \begin{tabular}{c|cc|cc|cc|cc|cc||cc}
     \hline
     \hline
     \multirow{2}*{\begin{tabular}{c}\textbf{Normalized}\\\textbf{Frequency}\end{tabular}} &
     \multicolumn{2}{c|}{\textbf{FIR}} &
     \multicolumn{2}{c|}{\textbf{Sobel}} &
     \multicolumn{2}{c|}{\textbf{IIR}} &
     \multicolumn{2}{c|}{\textbf{Butterworth}} &
     \multicolumn{2}{c||}{\textbf{DCT4}} &
     \multicolumn{2}{c}{\textbf{Geo.Mean}}\\
     &  {Uniform} & {Lena } &  {Uniform} & {Lena } &  {Uniform} & {Lena  }
     &  {Uniform} & {Lena } &  {Uniform} & {Lena } &  {Uniform} & {Lena}\\
     \hline
     1.04 & 99.85\%	& 100.00\%  & 99.51\% &	99.74\% &	 72.28\% &	90.09\% &	 79.03\% &	 100.00\% &	 83.58\% &	 98.06\% & 86.14\% & 97.50\%\\
     1.08 & 98.93\%	& 99.97\%	& 96.26\% &	93.75\% &	 71.64\% &	90.50\% &	 78.81\% &	 100.00\% &	 83.26\% &	 98.45\% & 85.15\% & 96.46\%\\
     1.12 & 94.27\%	& 98.82\%   & 96.25\% &	93.62\% &	 73.63\% &   88.25\% &	 81.88\% &	 84.87\%  &	 89.44\% &	 99.56\% & 86.68\% & 92.84\%\\
     1.16 & 99.66\%	& 99.91\%   & 73.73\% &	93.62\% &	 73.10\% &   89.92\% &	 79.30\% &	 84.23\%  &	 79.07\% &	 99.44\% & 80.44\% & 93.23\%\\
     1.20 & 96.03\%	& 99.90\%   & 81.55\% &	81.52\% &	 70.76\% &	75.67\% &	 64.96\% &	 84.50\%  &	 N/A\tnote{*}	  & N/A\tnote{*} & 77.46\% & 84.95\% \\
     1.24 & 98.46\%	& 99.32\%   & 81.43\% &	81.67\% &	 70.47\% &	76.12\% &	 63.66\% &	 84.23\%  &	 N/A\tnote{*}	  & N/A\tnote{*} & 77.44\% & 84.92\%\\
     1.28 & 95.39\%	& 99.29\%   & 60.41\% &	78.24\% &	 N/A\tnote{*}	    &   N/A\tnote{*}     &	54.38\% &	 75.15\%  &	N/A\tnote{*}	  & N/A\tnote{*} & 67.92\% & 83.58\%\\
     1.32 & 95.37\%	& 98.75\%   & N/A\tnote{*}	  &	 N/A\tnote{*}	    &	 N/A\tnote{*}	    &	 N/A\tnote{*}   & N/A\tnote{*}   & N/A\tnote{*}    &    N/A\tnote{*}   & N/A\tnote{*} & 95.37\%	& 98.75\% \\
     \hline
     \hline
   \end{tabular}
   \normalsize
   \begin{tablenotes}
   \footnotesize
    \item[*] Current frequency cannot be achieved in the traditional scenario. These points are excluded from the calculation of geometric means.
   \end{tablenotes}
   \end{threeparttable}
   \label{Ratio_MRE}
% \vspace{-1ex}
 \end{table*}

\begin{table*}[htbp]
 %\vspace{-2ex}
 %% increase table row spacing, adjust to taste
   \renewcommand{\arraystretch}{1.05}
   \setlength{\tabcolsep}{3.5pt}
   \caption{Frequency Speedups in Overclocking Scenario Under Various Error Budgets.}
%   \vspace{-2ex}
   \label{Exp_Real_Inputs}
   \centering
   \footnotesize
   \begin{tabular}{c|cc|cc|cc|cc|cc||cc}
     \hline
     \hline
     \multirow{2}*{\begin{tabular}{c}\textbf{Error Budget}\\\textbf{\%}\end{tabular}} &
     \multicolumn{2}{c|}{\textbf{FIR}} &
     \multicolumn{2}{c|}{\textbf{Sobel}} &
     \multicolumn{2}{c|}{\textbf{IIR}} &
     \multicolumn{2}{c|}{\textbf{Butterworth}} &
     \multicolumn{2}{c||}{\textbf{DCT4}} &
     \multicolumn{2}{c}{\textbf{Geo.Mean}} \\
     %\multirow{2}*{\begin{tabular}{c}\textbf{Geo.}\\\textbf{Mean}\end{tabular}}\\
     & {Uniform} & {Lena } &  {Uniform} & {Lena } &  {Uniform} & {Lena  }
     & {Uniform} & {Lena } &  {Uniform} & {Lena } &  {Uniform} & {Lena }\\
     \hline
     0.05 & 4.76\%&	21.43\%&	 6.82\%&	6.82\%&	0.95\%&	 1.26\%&	12.40\%&	 24.03\%&	 0.72\%&	 0.96\%  &3.07\%    &5.32\%\\
     0.5  & 19.05\%&	21.43\%&	 13.64\%&	6.82\%&	0.63\%&	 10.06\%&	24.03\%&	 24.03\%&	0.48\%&	 12.44\%   &4.52\% &13.45\%\\
     1    & 19.05\%&	28.57\%&	 13.64\%&	18.18\%&	 10.06\%&	16.35\%&	 24.03\%&	 24.03\%&	 0.48\%&	 12.44\%    &7.86\% &19.10\%\\
     5    & 19.15\%&	25.53\%&	 18.18\%&	18.18\%&	 0.54\%&	0.82\%&	 0.63\%&	 0.94\%&	 7.66\%&	 12.44\%    &3.91\%    &5.36\%\\
     10   & 19.15\%&	25.53\%&	 10.64\%&	10.64\%&	 0.54\%&	1.09\%&	 6.92\%&	 13.21\%&	 4.91\%&	 4.911\%    &5.19\%    &7.19\%\\
     20   & 19.15\%&	25.53\%&	 5.77\%&	15.39\%&	 8.70\%&	8.70\%&	 3.26\%&	 3.26\%&	 6.70\%&	 10.88\%    &7.32\%    &10.39\%\\
     50   & 15.69\%&	15.69\%&	 10.53\%&	19.30\%&	 42.50\%&	50.00\%&	 46.74\%&	 54.89\%&	 15.06\%&	 19.25\%    &21.8\% &27.59\%\\
     \hline
     \hline
   \end{tabular}
   \normalsize
   \label{Max_Frequency}
%   \vspace{-3ex}
 \end{table*}

Table~\ref{Max_Frequency} illustrates the frequency speedups for each design when the specified error tolerance varies from 0.05\% to 50\%. For all designs, we see that the overclocking scenario still outperforms the traditional scenario for each MRE budget in terms of operating frequency. Likewise, the frequency speedup is higher for real image inputs than uniform inputs. The geometric mean of frequency speedups of $3.1\%$ to $21.8\%$ can be achieved by using uniform data, while $5.3\%$ to $27.6\%$ when using real image data.

\subsection{Area Overhead of Our Approach}
For both aforementioned design goals, it should be noted that an extra benefit of using the traditional approach is that reducing datapath precision would potentially leads to a smaller design, in comparison to our proposed overclocking scenario. Consequently, the area overheads of our approach are summarized in Table~\ref{Table_AreaFrequency} and Table~\ref{Table_AreaError} for both design goals. In our experiments, the area overhead is evaluated in terms of the number of LUTs and Slices in the FPGA technology, while similar metrics can also be employed if applying our approach into other hardware platforms. It can be seen that for a given frequency requirement with minimum possible errors, the geometric mean of area overhead ranges from $1.16\times$ and $1.15\times$ to $5.57\times$ and $4.46\times$ for LUTs and Slices, respectively.

An even smaller area overhead can be found for a specified error budget with the maximum clock frequencies. As seen in Table~\ref{Table_AreaFrequency}, the geometric mean of area overhead in terms of LUTs and Slices is $1.00\times\sim4.67\times$ and $1.00\times\sim3.70\times$ respectively. Notice that an area overhead of $1\times$ means both the overclocking scenario and the traditional scenario use the original precision, however the former achieves frequency speedups as seen in Table~\ref{Table_AreaError}, because a specific error budget can be tolerated.

In general, we can observe a relatively small area overhead of our approach, especially when frequency is initially increased or with a small error budget (i.e. $\sim10\%$).

\begin{table*}[htbp]
 %% increase table row spacing, adjust to taste
   \renewcommand{\arraystretch}{1.05}
   \setlength{\tabcolsep}{3.5pt}
   \caption{Area Overhead of Our Approach with respect to Given Frequency Requirements}
   \label{Table_AreaFrequency}
   \centering
   \footnotesize
   \begin{threeparttable}
   \begin{tabular}{c|cc|cc|cc|cc|cc||cc}
     \hline
     \hline
     \multirow{2}*{\begin{tabular}{c}\textbf{Normalized}\\\textbf{Frequency}\end{tabular}} &
     \multicolumn{2}{c|}{\textbf{FIR}} &
     \multicolumn{2}{c|}{\textbf{Sobel}} &
     \multicolumn{2}{c|}{\textbf{IIR}} &
     \multicolumn{2}{c|}{\textbf{Butterworth}} &
     \multicolumn{2}{c||}{\textbf{DCT4}} &
     \multicolumn{2}{c}{\textbf{Geo.Mean}}\\
     &  {LUTs} & {Slices } &  {LUTs} & {Slices } &  {LUTs} & {Slices  }
     &  {LUTs} & {Slices } &  {LUTs} & {Slices } &  {LUTs} & {Slices}\\
     \hline
     1.04 & 1.24	& 1.05  & 1.12 &	1.22 &	 1.26 &	1.42 &	 1.16 &	 1.11 &	 1.13 &	 1.00 & 1.18 & 1.15\\
     1.08 & 1.24	& 1.05	& 1.03 &	1.08 &	 1.26 &	1.42 &	 1.16 &	 1.11 &	 1.11 &	 1.15 & 1.16 & 1.16\\
     1.12 & 1.24	& 1.05   & 1.03 &	1.08 &	 1.26 & 1.42 &	 1.37 &	1.54  &	 2.30 &	 2.17 & 1.38 & 1.40\\
     1.16 & 2.95	& 2.56   & 1.03 &	1.08 &	 1.61 & 1.71 &	 1.99 &	 2.03  &	 4.60 &	 2.89 &2.14 & 1.95\\
     1.20 & 2.95	& 2.56   & 1.66 &	1.39 &	 2.77 &	3.48 &	3.17 &	 3.50  &	 N/A\tnote{*}	  & N/A\tnote{*} & 2.56 & 2.57 \\
     1.24 & 4.42	& 3.15  & 4.00 &	3.55 &	 7.87 &	6.73 &	6.92 &	 5.25  &	 N/A\tnote{*}	  & N/A\tnote{*} & 5.57 & 4.46\\
     1.28 & 4.42	& 3.15   & 4.00 &	3.55 &	 N/A\tnote{*}	    &   N/A\tnote{*}     &	6.92 &	 5.25  &	N/A\tnote{*}	  & N/A\tnote{*} & 4.97& 3.89\\
     1.32 & 4.42	& 3.15   & N/A\tnote{*}	  &	 N/A\tnote{*}	    &	 N/A\tnote{*}	    &	 N/A\tnote{*}   & N/A\tnote{*}   & N/A\tnote{*}    &    N/A\tnote{*}   & N/A\tnote{*} & 4.42	& 3.15 \\
     \hline
     \hline
   \end{tabular}
   \normalsize
   \begin{tablenotes}
   \footnotesize
    \item[*] Current frequency cannot be achieved in the traditional scenario. These points are excluded from the calculation of geometric means.
   \end{tablenotes}
   \end{threeparttable}
   \label{Ratio_MRE}
 \end{table*}

 \begin{table*}[htbp]
 %% increase table row spacing, adjust to taste
   \renewcommand{\arraystretch}{1.05}
   \setlength{\tabcolsep}{3.5pt}
   \caption{Area Overhead of Our Approach with respect to Given Error Budgets.}
   \label{Table_AreaError}
   \centering
   \footnotesize
   \begin{tabular}{c|cc|cc|cc|cc|cc||cc}
     \hline
     \hline
     \multirow{2}*{\begin{tabular}{c}\textbf{Error Budget}\\\textbf{\%}\end{tabular}} &
     \multicolumn{2}{c|}{\textbf{FIR}} &
     \multicolumn{2}{c|}{\textbf{Sobel}} &
     \multicolumn{2}{c|}{\textbf{IIR}} &
     \multicolumn{2}{c|}{\textbf{Butterworth}} &
     \multicolumn{2}{c||}{\textbf{DCT4}} &
     \multicolumn{2}{c}{\textbf{Geo.Mean}} \\
     %\multirow{2}*{\begin{tabular}{c}\textbf{Geo.}\\\textbf{Mean}\end{tabular}}\\
     & {LUTs} & {Slices } &  {LUTs} & {Slices } &  {LUTs} & {Slices }
     & {LUTs} & {Slices } &  {LUTs} & {Slices } &  {LUTs} & {Slices }\\
     \hline
     0.05 & 1.00&	1.00& 1.00&	1.00& 1.00&	 1.00&	1.00&	 1.00&	1.00&	 1.00  &1.00  &  1.00\\
     0.5  & 1.02&	1.15& 1.00&	1.00& 1.00&	 1.00&	1.00&	 1.00&	1.00&	 1.00   &1.00 &1.03\\
     1    & 1.02&	1.15& 1.00&	1.00& 1.00&	 1.00&	1.00&	 1.00&	1.00&	 1.00   &1.00 &1.03\\
     5    & 1.24&	1.05& 1.00&	1.00& 1.26&   1.42&   1.65&   1.11&	 1.00&    1.00   &1.13    &1.11\\
     10   & 1.24&	1.05& 1.12&	1.22& 1.26&	1.42& 1.65&	1.11& 1.11&	 1.15&    1.18    &1.18\\
     20   & 1.24&	1.05& 1.03&	1.08& 1.26&	1.42& 6.92&	 5.25&   3.45&	 2.17   &2.07    &1.79\\
     50   & 2.95&	2.56& 4.00&	3.55& 7.87&	6.73& 6.92&	 5.25&   3.45&	 2.12	&4.67 &3.70\\
     \hline
     \hline
   \end{tabular}
   \normalsize
   \label{Max_Frequency}
 \end{table*}

\section{Conclusion}  \label{Section_Conclusion}
The conclusion goes here.

\section*{Acknowledgment}
The authors would like to acknowledge the support of the EPSRC (Grants EP/I020557/1 and EP/I012036/1).


%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results.}
%\label{fig_sim}
%\end{figure}


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}




% references section
\bibliographystyle{./IEEEtran}
\bibliography{./IEEEabrv,./Reference}


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Author}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{Author}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Author}
Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\end{document}


